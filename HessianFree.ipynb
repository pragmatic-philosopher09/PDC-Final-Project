{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5de30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Dropout,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.optimizers import SGD,Adam,RMSprop\n",
    "import pandas as pd\n",
    "import time\n",
    "from tensorflow.keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e22f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hessianfree in d:\\anaconda\\lib\\site-packages (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hessianfree\n",
    "import hessianfree as hf\n",
    "from hessianfree.loss_funcs import LossFunction\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2b9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TURN GPU for Keras, set devic = cuda or gpu or gpu0 like this\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cuda,openmp=1,floatX=float32\" \n",
    "# TO TURN ON OPENMP\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cpu,openmp=1,floatX=float32\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5136ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_NN(n_nodes,optimizer):\n",
    "    '''This function initializes and return a new neural network with regularization techniques\n",
    "       \n",
    "       input: \n",
    "       n_nodes: a list of units per layer like [42,24,12,1] \n",
    "       optimizer: one of the following:\n",
    "        sgd = SGD\n",
    "        rmsprop = RMSprop\n",
    "        adagrad = Adagrad\n",
    "        adadelta = Adadelta\n",
    "        adam = Adam\n",
    "        adamax = Adamax\n",
    "        nadam = Nadam\n",
    "       \n",
    "\n",
    "       output: an object that contains these methods:\n",
    "       \n",
    "       model.predict(X): return predictions corresponding to X\n",
    "       \n",
    "       model.get_weights(): return a list of current model weights, in the order of w0,b1,w1,b1,....w4,b4\n",
    "       \n",
    "       model.set_weights(): takes in a list of weights in the same format as what model.get_weights() returns\n",
    "       \n",
    "       model.fit(X_tr,Y_tr,verbose=0,epochs=50,batch_size=1024,validation_split=0.2, callbacks=[early_stopping]): \n",
    "       \n",
    "       train a model with the inputs and the specification, you can train 1 epoch;  \n",
    "       and return history of loss during training (using hist.history['loss']) and validation loss if callbacks =\n",
    "       [EarlyStopping(patience=5)] (using hist.history['val_loss']) \n",
    "       \n",
    "    '''\n",
    "    # Clear the model\n",
    "    model = None\n",
    "    # BUILD INPUT LAYER\n",
    "    inputs = Input(shape=(n_nodes[0],))\n",
    "\n",
    "    # CONNECT TO THE FIRST HIDDEN LAYER\n",
    "    x = Dense(n_nodes[1], kernel_initializer='he_normal', \n",
    "                    kernel_regularizer=l2(0.0001),kernel_constraint = max_norm(5), activation='relu')(inputs)\n",
    "    x = Dropout(0.2)(x) # add dropout \n",
    "\n",
    "    # ADD SOME MORE HIDDEN LAYERS\n",
    "    for i in range(2,len(n_nodes)-1):\n",
    "        x = Dense(n_nodes[i],  kernel_initializer='he_normal', activation='relu',bias_initializer='he_normal',\n",
    "            kernel_regularizer=l2(0.0001),kernel_constraint = max_norm(3))(x)\n",
    "        x = Dropout(0.2)(x) # add dropout \n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    predictions = Dense(1, kernel_initializer='he_normal', activation='linear')(x)\n",
    "\n",
    "    # INITIALIZE MODEL (now you can call model.get_weights() )\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # Compile model with LOSS FUNCTION and ADAM OPTIMIZER\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c023ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of total X and ret: (19669, 42) (19669, 1)\n"
     ]
    }
   ],
   "source": [
    "# Example OF comparing keras and Hessian Free: \n",
    "\n",
    "# read data and define training, validation and test set\n",
    "data = np.genfromtxt('price_inputs_GS2016.csv',delimiter=',',skip_header=1)\n",
    "X,ret = data[:,2:],data[:,1:2] # X means features, ret means target \n",
    "print('shape of total X and ret:',X.shape,ret.shape)\n",
    "\n",
    "n_test = int(X.shape[0]*0.25)\n",
    "N = X.shape[0] - n_test\n",
    "n_val = int(N*0.2)\n",
    "X_tr_temp, X_test, ret_tr_temp,ret_test = X[:-n_test],X[-n_test:],ret[:-n_test],ret[-n_test:]\n",
    "X_tr,X_val,ret_tr,ret_val = X_tr_temp[:-n_val], X_tr_temp[-n_val:],ret_tr_temp[:-n_val],ret_tr_temp[-n_val:]\n",
    "\n",
    "\n",
    "# define evaluation metrics\n",
    "accuracy = lambda pred,truth: np.mean((pred>0)==(truth>0))\n",
    "hit_ratio = lambda x,y: np.mean( ((x[1:] - x[:-1]) * (y[1:]-y[:-1]))>0 )\n",
    "eval_f = [accuracy,hit_ratio,mean_squared_error,mean_absolute_error]\n",
    "labels = 'accuracy,hit_ratio,mean_squared_error,mean_absolute_error'.split(',')\n",
    "\n",
    "n_trials = 1 # run some number of trials for each model for confidence interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9d890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fitting on the training set for 100 epochs, keras return this weight parameter\n",
      "[array([[-0.11677323,  0.03750786, -0.02175612, ..., -0.05536552,\n",
      "        -0.06611554, -0.05894994],\n",
      "       [ 0.02889547, -0.06271525,  0.07103355, ...,  0.13097164,\n",
      "         0.4106272 ,  0.13271786],\n",
      "       [ 0.11421447,  0.11550514, -0.03584766, ..., -0.29455692,\n",
      "         0.25533745, -0.14664645],\n",
      "       ...,\n",
      "       [ 0.1610842 , -0.21801807, -0.17005669, ...,  0.28941494,\n",
      "        -0.08052007,  0.19307862],\n",
      "       [ 0.30354312, -0.06030658, -0.05299408, ...,  0.18896367,\n",
      "        -0.04619488, -0.31426293],\n",
      "       [ 0.01657236, -0.06964859, -0.3642317 , ...,  0.27775666,\n",
      "        -0.35603642,  0.3276148 ]], dtype=float32), array([ 0.02553064, -0.01172137, -0.05070024, -0.048676  , -0.03211929,\n",
      "       -0.00596489, -0.02388897,  0.00882523, -0.03264376,  0.00826216,\n",
      "        0.03675783, -0.01347234,  0.02514146, -0.03406848,  0.0194388 ,\n",
      "       -0.00542165,  0.00380377, -0.00282063,  0.01813933, -0.04519471,\n",
      "       -0.00174475, -0.00956389, -0.01211199, -0.08081029], dtype=float32), array([[ 0.01733796,  0.61052704, -0.33448055, -0.2273474 , -0.3509503 ,\n",
      "         0.38681316, -0.31596857, -0.12566136, -0.15935577, -0.2696806 ,\n",
      "        -0.36564434,  0.05450522],\n",
      "       [-0.02390276, -0.13873123,  0.14736255, -0.20285502,  0.166559  ,\n",
      "        -0.12134163, -0.05832767, -0.51043916,  0.03196799, -0.27969396,\n",
      "         0.2868074 , -0.06523369],\n",
      "       [ 0.19436334,  0.3042561 ,  0.08697808, -0.00383663, -0.1389999 ,\n",
      "        -0.203578  , -0.02349694, -0.01336664, -0.19855092,  0.00129599,\n",
      "         0.10097494,  0.19829103],\n",
      "       [-0.0422467 , -0.42367885,  0.31408608, -0.08511431, -0.5215093 ,\n",
      "         0.51140785, -0.63170385,  0.15042166,  0.02544416, -0.14305681,\n",
      "        -0.21695215,  0.09284416],\n",
      "       [ 0.17816037, -0.23769319,  0.18833864,  0.6116518 ,  0.45664784,\n",
      "        -0.4070599 ,  0.09527365, -0.13359591,  0.63040006, -0.2013454 ,\n",
      "        -0.34503967,  0.0189188 ],\n",
      "       [-0.1155491 ,  0.36963627, -0.1701374 ,  0.09317181, -0.32893026,\n",
      "        -0.11175383, -0.37681183,  0.14409748, -0.30127656,  0.368159  ,\n",
      "        -0.12812921,  0.08494806],\n",
      "       [ 0.09342726,  0.25343293, -0.42328045,  0.1477349 , -0.03882574,\n",
      "         0.33138603,  0.06022754,  0.27150783, -0.5759994 ,  0.18186224,\n",
      "         0.02297412,  0.20729558],\n",
      "       [ 0.08852335, -0.56366247, -0.348135  ,  0.11607173,  0.14029022,\n",
      "        -0.33106226,  0.01979902,  0.6303166 ,  0.11727861, -0.39337093,\n",
      "        -0.31676146, -0.44928274],\n",
      "       [-0.07160591,  0.05417126,  0.05468937,  0.21386091, -0.42260885,\n",
      "         0.22630788,  0.06708695, -0.1836541 , -0.23894688,  0.12704797,\n",
      "         0.36072788,  0.43334234],\n",
      "       [-0.32431212,  0.38861808, -0.56803566,  0.07527421,  0.1695484 ,\n",
      "         0.19597454, -0.04382361, -0.29402283, -0.0071457 ,  0.12901315,\n",
      "        -0.28377467, -0.03700016],\n",
      "       [-0.25005683,  0.12640566, -0.00145704,  0.04241715,  0.24193244,\n",
      "         0.26020715, -0.6255026 , -0.19741371, -0.48301905, -0.43279606,\n",
      "        -0.14167805, -0.01234755],\n",
      "       [ 0.12022952,  0.5070474 ,  0.16781662,  0.22390987,  0.285832  ,\n",
      "         0.19223036, -0.02585657,  0.44147512, -0.26620334, -0.13040972,\n",
      "         0.16875039, -0.21893658],\n",
      "       [-0.489675  ,  0.3159045 ,  0.02526304, -0.24412014,  0.10967709,\n",
      "         0.3100914 ,  0.20100546, -0.00740571, -0.1379992 , -0.35688427,\n",
      "         0.21648318, -0.44120234],\n",
      "       [-0.24064456,  0.05701732,  0.37885115,  0.2573681 , -0.03572239,\n",
      "        -0.6252165 , -0.17606868, -0.02156659, -0.29009414,  0.0302684 ,\n",
      "         0.39251104, -0.15213536],\n",
      "       [-0.5273962 ,  0.5698538 ,  0.03672883,  0.23489557,  0.22114234,\n",
      "        -0.24690276, -0.35042298,  0.22626029, -0.13391873, -0.06328962,\n",
      "         0.39791644,  0.31143448],\n",
      "       [-0.31001642,  0.36387143, -0.2003208 , -0.4345523 ,  0.49708197,\n",
      "        -0.08618687, -0.02809896, -0.2552846 ,  0.26271808,  0.19409378,\n",
      "         0.2861915 ,  0.13031168],\n",
      "       [-0.10456429,  0.35906023, -0.26164454,  0.27585572,  0.07143094,\n",
      "        -0.10117371, -0.22557536, -0.18059881,  0.2641672 ,  0.02277369,\n",
      "         0.05584265, -0.36919403],\n",
      "       [-0.18182077,  0.06471488, -0.62737626, -0.21594565,  0.09259374,\n",
      "         0.09358032,  0.60770833,  0.50709736, -0.44145042,  0.04892555,\n",
      "        -0.0055632 , -0.34057322],\n",
      "       [-0.28589323, -0.3082948 ,  0.05607847, -0.38759056,  0.13722256,\n",
      "         0.09427243,  0.09440606, -0.5729732 , -0.2879612 , -0.2494272 ,\n",
      "        -0.25137126, -0.20342577],\n",
      "       [ 0.07030818,  0.30250427,  0.08336977, -0.0467542 ,  0.3847277 ,\n",
      "         0.24076752,  0.2024622 , -0.12795617, -0.07525593, -0.01449673,\n",
      "        -0.13368666, -0.60668844],\n",
      "       [-0.42983052, -0.49677306, -0.16347204,  0.21953267,  0.34961084,\n",
      "        -0.22316411, -0.05452355, -0.39569208, -0.17656778,  0.37311462,\n",
      "        -0.0155999 ,  0.22362603],\n",
      "       [-0.02029227,  0.06075621, -0.2716284 , -0.3428064 ,  0.19022259,\n",
      "        -0.12041626,  0.5751242 , -0.3136787 , -0.09853441, -0.03536402,\n",
      "         0.4255599 ,  0.313856  ],\n",
      "       [-0.07930093, -0.31247878,  0.39850324, -0.27073067, -0.35969168,\n",
      "         0.03230285, -0.41277203, -0.11882286, -0.06174742, -0.15991624,\n",
      "         0.04417744, -0.33654597],\n",
      "       [ 0.28671163,  0.09407596,  0.2973217 ,  0.08721434,  0.3328327 ,\n",
      "        -0.15614216, -0.2079999 ,  0.19971852,  0.14977816,  0.05038263,\n",
      "         0.11533847,  0.35542312]], dtype=float32), array([ 0.27942425, -0.18941164, -0.90534973, -0.9244967 , -0.25149232,\n",
      "        0.12271061, -0.75226647, -0.521211  , -0.24777743, -0.07927127,\n",
      "       -0.57901525, -0.39081344], dtype=float32), array([[-0.70735645],\n",
      "       [ 0.0079071 ],\n",
      "       [-0.393992  ],\n",
      "       [-0.05122043],\n",
      "       [ 0.00622872],\n",
      "       [ 0.00142114],\n",
      "       [ 0.16602144],\n",
      "       [ 0.346616  ],\n",
      "       [-0.31543675],\n",
      "       [ 0.2168462 ],\n",
      "       [ 0.11681413],\n",
      "       [-0.05200257]], dtype=float32), array([-0.02595935], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "################### KERAS ONLY ######################\n",
    "\n",
    " \n",
    "# define hyperparameters\n",
    "n_nodes = [42,24,12,1] # number of units per layer\n",
    "batch_size = 1024\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "# CHOOSE adam or adagrad \n",
    "model = keras_NN(n_nodes=n_nodes,optimizer='sgd')\n",
    "model.fit(X_tr,ret_tr,verbose=0,epochs=100,batch_size=batch_size,\n",
    "                 validation_data=(X_val,ret_val),callbacks=[early_stopping])\n",
    "print('After fitting on the training set for 100 epochs, keras return this weight parameter') \n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cc630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\hessianfree\\ffnet.py:432: UserWarning: Input dtype (float64) not equal to self.dtype (<class 'numpy.float32'>)\n",
      "  warnings.warn(\"Input dtype (%s) not equal to self.dtype (%s)\" %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fitting on the training set for 100 epochs, hessian free return this weight parameter\n",
      "[array([[ 0.8292594 , -0.5707942 , -0.44953394, ...,  0.09422408,\n",
      "         0.8934627 ,  0.6504265 ],\n",
      "       [ 1.2634635 ,  0.08433252, -0.53944516, ...,  0.9171897 ,\n",
      "         0.74347943,  1.1688513 ],\n",
      "       [-1.3517568 ,  1.4753577 ,  0.41765174, ..., -0.73805666,\n",
      "        -0.75450635, -0.6591747 ],\n",
      "       ...,\n",
      "       [ 0.91960186, -0.9872238 , -0.68048793, ...,  0.45513633,\n",
      "         1.220806  , -0.7912785 ],\n",
      "       [-0.36187336, -0.73655057,  0.4322151 , ..., -0.3468003 ,\n",
      "        -1.585897  , -0.03222641],\n",
      "       [-0.53680503, -0.2690873 , -0.69392437, ..., -0.2054766 ,\n",
      "        -0.53520006, -0.21770248]], dtype=float32), array([1.0585736 , 0.8601018 , 0.9611594 , 1.045068  , 1.0495595 ,\n",
      "       0.920602  , 0.91276264, 0.8937233 , 1.0429558 , 1.071606  ,\n",
      "       0.99281   , 0.9316847 , 1.0238755 , 1.0735124 , 1.1104681 ,\n",
      "       1.2050701 , 1.0519226 , 1.0810913 , 1.0407579 , 1.0515708 ,\n",
      "       0.9310919 , 1.0862433 , 0.9285586 , 0.71847045], dtype=float32), array([[-1.3521236 ,  0.04592004,  1.3026304 , -0.5271038 , -1.822829  ,\n",
      "        -0.01123798,  0.10179533,  0.76985615, -0.7939199 ,  2.4701884 ,\n",
      "        -1.4722159 ,  0.2683148 ],\n",
      "       [ 1.8848221 ,  0.60441333, -0.5289066 ,  1.4972953 ,  0.4492371 ,\n",
      "         0.05925208,  0.49221942, -0.30632496, -0.258417  , -0.28715497,\n",
      "        -0.75434136,  0.91435856],\n",
      "       [-0.32645828,  0.27018818, -1.147575  , -0.71288764,  0.7667876 ,\n",
      "        -0.5674429 ,  0.32713902, -0.50102776,  0.43532413,  1.0980966 ,\n",
      "        -0.3811758 , -1.4230895 ],\n",
      "       [-1.0753437 ,  0.31614783,  1.0262194 ,  1.6046269 ,  0.32796386,\n",
      "         0.23962933, -1.3897252 ,  0.04149169, -0.08379833,  0.2683747 ,\n",
      "         0.4098669 , -2.8737226 ],\n",
      "       [-1.5940849 , -1.3003427 ,  0.6802976 , -1.0331064 , -0.22251046,\n",
      "        -0.24275431,  0.5415787 , -1.0956429 , -1.0216947 ,  1.223298  ,\n",
      "         1.8641403 , -1.0121213 ],\n",
      "       [-0.63038224,  1.8788526 ,  1.1321498 ,  0.8750081 ,  1.0529673 ,\n",
      "         0.9766689 ,  0.25145462,  0.15686193, -0.7691106 , -1.321972  ,\n",
      "         0.8242084 , -0.99966675],\n",
      "       [ 1.3283067 ,  0.12368345, -0.18099427, -0.7573747 ,  1.4474185 ,\n",
      "        -2.2054245 ,  0.91795576, -0.5635    ,  0.28974193,  0.01778645,\n",
      "        -0.5204026 , -0.7863524 ],\n",
      "       [ 1.2763202 , -0.98094696,  1.1187288 ,  0.45046192,  2.170197  ,\n",
      "         0.40374848,  1.278549  , -0.3871975 ,  0.40317973,  1.6159184 ,\n",
      "         1.8345268 , -0.29791662],\n",
      "       [-0.70417583, -1.1956477 ,  0.646341  , -0.16534153,  0.53688264,\n",
      "         0.2110939 ,  2.5173275 , -0.00654649, -1.3430419 , -2.1402125 ,\n",
      "        -0.5396318 ,  1.6584922 ],\n",
      "       [-0.80123967, -1.2450142 , -0.53255   ,  0.69523925, -0.57605374,\n",
      "         1.0823699 , -0.888323  ,  0.8592136 ,  0.8844441 , -0.37069145,\n",
      "        -1.8289105 , -0.01720488],\n",
      "       [ 1.1493479 , -0.6651021 , -1.6000911 , -0.5217774 ,  0.15548493,\n",
      "         0.65203255,  2.5513124 , -1.4020629 ,  1.0337963 ,  0.802613  ,\n",
      "        -0.92510706, -0.10546629],\n",
      "       [-0.19458392,  0.63336307,  1.0037223 , -0.55943847,  0.6520111 ,\n",
      "         0.58766156, -1.7115796 , -1.7138977 , -0.16458504,  0.05706213,\n",
      "        -0.57459146,  0.5173331 ],\n",
      "       [-0.6641521 , -0.3942796 , -0.07101877, -0.04612727,  0.23676018,\n",
      "         0.7440707 , -0.6239762 ,  0.84316856, -0.23976226,  0.34711114,\n",
      "         0.6053918 ,  0.15038745],\n",
      "       [-0.02037326, -0.23073383, -1.3236654 ,  1.4982163 ,  0.48093966,\n",
      "         0.6120265 , -1.1957155 ,  0.91040677, -0.5927375 ,  2.1791139 ,\n",
      "         1.9737487 ,  0.2671753 ],\n",
      "       [-0.4390361 , -0.8181154 , -0.55596673, -0.26962093,  1.081182  ,\n",
      "         0.38937342,  0.27669677, -0.23926947, -1.8702667 , -1.6056758 ,\n",
      "        -0.21974662,  1.1676021 ],\n",
      "       [-1.4663111 , -0.7801097 , -0.5065367 ,  2.0647116 ,  0.6443    ,\n",
      "         0.37063077,  1.1663914 ,  0.48463675, -1.127218  ,  0.14143868,\n",
      "         0.35104007,  0.25780433],\n",
      "       [ 1.092889  , -1.5745137 , -1.7485825 ,  1.3248148 , -1.7584108 ,\n",
      "        -1.243804  , -1.1368821 , -1.4653264 ,  0.23432821,  1.8676814 ,\n",
      "         0.7873271 , -1.3263137 ],\n",
      "       [-1.3759415 , -0.21178555,  0.02551701,  0.59717584, -0.87799597,\n",
      "        -0.33767223,  0.7939944 , -0.01755527,  1.0388504 , -0.06779277,\n",
      "         0.75999767, -1.3004957 ],\n",
      "       [-0.8228232 ,  1.4314237 , -0.52424604,  0.5461503 , -0.85788786,\n",
      "         0.16708992,  0.71348864,  2.1156673 ,  0.34901154,  0.65145844,\n",
      "         1.5445578 ,  0.6199129 ],\n",
      "       [-0.40811902, -1.8737757 , -0.29636264,  0.35137063,  0.11011595,\n",
      "        -2.4397058 , -0.04928068, -1.3553929 ,  0.3549837 ,  0.9601423 ,\n",
      "        -0.53763837,  0.38361207],\n",
      "       [-0.66205513,  0.98158485,  1.1898123 ,  0.0256749 ,  0.5155315 ,\n",
      "        -1.0126281 ,  1.556198  ,  0.2284605 ,  0.73056537,  0.91431457,\n",
      "         0.31090966,  0.9393842 ],\n",
      "       [ 0.34999603,  0.5428235 , -1.3000793 , -1.3920697 ,  0.7547684 ,\n",
      "        -0.15680455,  0.5587725 , -0.44072327, -0.6844398 ,  0.7928006 ,\n",
      "        -0.802959  , -2.5885794 ],\n",
      "       [ 0.22951367,  2.7792182 , -0.15741909,  0.10184599, -0.54828   ,\n",
      "         0.5210923 , -2.1292715 ,  0.3892516 , -0.12369278, -0.79981256,\n",
      "        -0.6150827 ,  1.1921546 ],\n",
      "       [ 1.5922838 ,  0.36131847, -1.1140366 , -1.2911333 , -0.55506295,\n",
      "         0.09106055,  0.8102855 ,  0.29175383,  1.4243412 ,  1.6369663 ,\n",
      "        -1.4053075 , -1.0687196 ]], dtype=float32), array([0.90897423, 0.9908858 , 0.9097172 , 1.004731  , 0.99649787,\n",
      "       0.9983553 , 1.0100048 , 0.9916994 , 0.9674295 , 0.9993646 ,\n",
      "       0.9925459 , 0.98944604], dtype=float32), array([[-1.9320449e+00],\n",
      "       [-1.8602287e-03],\n",
      "       [ 1.7221824e+00],\n",
      "       [ 3.0848915e-03],\n",
      "       [-2.3308366e-03],\n",
      "       [-8.5736234e-03],\n",
      "       [-3.6969138e-03],\n",
      "       [-9.8115187e-03],\n",
      "       [ 1.9826193e-01],\n",
      "       [-3.0588848e-04],\n",
      "       [ 7.1057891e-03],\n",
      "       [-1.0228027e-02]], dtype=float32), array([0.01450051], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "################### Hessian Free ######################\n",
    "\n",
    "\n",
    "\n",
    "def output_loss(func):\n",
    "    \"\"\"Convenience decorator that takes a loss defined for the output layer\n",
    "    and converts it into the more general form in terms of all layers.\"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapped_loss(self, activities, targets):\n",
    "        result = [None for _ in activities[:-1]]\n",
    "        result += [func(self, activities[-1], targets)]\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapped_loss\n",
    "\n",
    "class mse(LossFunction):\n",
    "    \n",
    "    @output_loss\n",
    "    def loss(self, output, targets):\n",
    "        return np.sum(np.nan_to_num(output - targets) ** 2,\n",
    "                      axis=tuple(range(1, output.ndim))) / 2 /output.shape[0]\n",
    "\n",
    "    @output_loss\n",
    "    def d_loss(self, output, targets):\n",
    "        return np.nan_to_num(output - targets)/output.shape[0]\n",
    "\n",
    "    @output_loss\n",
    "    def d2_loss(self, output, _):\n",
    "        return np.ones_like(output)/output.shape[0]\n",
    "    \n",
    "def pack_weights(ff):\n",
    "    '''\n",
    "    input: an hessian free model\n",
    "    output: a list of weight following keras' format\n",
    "    ff follows this format: [(W_0,b_0),(W_1,b_1)...(W_H,b_H)]'''\n",
    "    res = []\n",
    "    for i in range(len(n_nodes)-1):\n",
    "        weights = ff.get_weights(ff.W,(i,i+1))\n",
    "        \n",
    "        res.extend([np.array(weights[0]),np.array(weights[1])])\n",
    "    return res\n",
    "\n",
    "pshape = lambda a_list: [ w.shape for w in a_list]\n",
    "\n",
    "\n",
    "# define hyperparameters\n",
    "layers = (len(n_nodes)-1)*['ReLU'] + ['Linear'] # all relu except linear for output layer\n",
    "n_nodes = [42,24,12,1] # number of units per layer\n",
    "batch_size = 1024\n",
    "\n",
    "\n",
    "# initialize a hessian free model with GPU use optional\n",
    "ff = hf.FFNet(n_nodes,layers=layers,loss_type=mse(),\n",
    "          W_init_params={ \"coeff\":1.0, \"biases\":1.0,\"init_type\":'gaussian'},use_GPU=0)\n",
    "\n",
    "ff.run_epochs(X,ret,test=(X_val,ret_val),minibatch_size=1024,\n",
    "                      optimizer=hf.opt.HessianFree(CG_iter=2),\n",
    "                      max_epochs=50, plotting=True,print_period=None)\n",
    "\n",
    "print('After fitting on the training set for 100 epochs, hessian free return this weight parameter') \n",
    "print(pack_weights(ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94fe7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\hessianfree\\ffnet.py:432: UserWarning: Input dtype (float64) not equal to self.dtype (<class 'numpy.float32'>)\n",
      "  warnings.warn(\"Input dtype (%s) not equal to self.dtype (%s)\" %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step\n",
      "Evaluating  keras adagrad\n",
      "running time per trial [4.99666834]\n",
      "prediction scores\n",
      "accuracy\n",
      "2.5 and 97.5 percentile [0.49928818385194224,0.49928818385194224]\n",
      "hit_ratio\n",
      "2.5 and 97.5 percentile [0.4420260374288039,0.4420260374288039]\n",
      "mean_squared_error\n",
      "2.5 and 97.5 percentile [1.1620071253546367,1.1620071253546367]\n",
      "mean_absolute_error\n",
      "2.5 and 97.5 percentile [0.7196781295589493,0.7196781295589493]\n",
      "Evaluating  hessian free\n",
      "running time per trial [4.99828696]\n",
      "prediction scores\n",
      "accuracy\n",
      "2.5 and 97.5 percentile [0.49928818385194224,0.49928818385194224]\n",
      "hit_ratio\n",
      "2.5 and 97.5 percentile [0.49125305126118796,0.49125305126118796]\n",
      "mean_squared_error\n",
      "2.5 and 97.5 percentile [5.6297867114931,5.6297867114931]\n",
      "mean_absolute_error\n",
      "2.5 and 97.5 percentile [1.471740810557929,1.471740810557929]\n"
     ]
    }
   ],
   "source": [
    "############################## Evaluation metrics ##############################\n",
    "\n",
    "# run some number of trials for each model\n",
    "n_trials = 1\n",
    "n_nodes = [42,24,12,1] # number of units per layer\n",
    "batch_size = 1024\n",
    "layers = (len(n_nodes)-1)*['ReLU'] + ['Linear'] # all relu except linear for output layer\n",
    "\n",
    "# define evaluation metrics\n",
    "accuracy = lambda pred,truth: np.mean((pred>0)==(truth>0))\n",
    "hit_ratio = lambda x,y: np.mean( ((x[1:] - x[:-1]) * (y[1:]-y[:-1]))>0 )\n",
    "eval_f = [accuracy,hit_ratio,mean_squared_error,mean_absolute_error]\n",
    "labels = 'accuracy,hit_ratio,mean_squared_error,mean_absolute_error'.split(',')\n",
    "\n",
    "timer = np.zeros((n_trials,2))\n",
    "scores = np.zeros( (n_trials,len(labels), 2) )\n",
    "\n",
    "for i in range(n_trials):\n",
    "                      \n",
    "    # CHOOSE sgd, adam or adagrad \n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    start = time.time()\n",
    "    model = keras_NN(n_nodes=n_nodes,optimizer='sgd')\n",
    "    hist = model.fit(X_tr,ret_tr,verbose=0,epochs=100,batch_size=batch_size,\n",
    "                     validation_data=(X_val,ret_val),callbacks=[early_stopping])\n",
    "    timer[i,0] = time.time()-start\n",
    "    \n",
    "    # evaluation metrics\n",
    "    pred = model.predict(X_test).flatten()\n",
    "    truth = ret_test.flatten()\n",
    "    scores[i,:,0] = [ f(pred,truth) for j,f in enumerate(eval_f) ]\n",
    "               \n",
    "    \n",
    "    # initliaze a hessian free model\n",
    "    ff = hf.FFNet(n_nodes,layers=layers,loss_type=hf.loss_funcs.SquaredError(),\n",
    "              W_init_params={ \"coeff\":1.0, \"biases\":1.0,\"init_type\":'gaussian'},use_GPU=0)\n",
    "    \n",
    "    # Hession free\n",
    "    start = time.time()\n",
    "    ff.run_epochs(X,ret,test=(X_val,ret_val),minibatch_size=1024,\n",
    "                          optimizer=hf.opt.HessianFree(CG_iter=2),\n",
    "                          max_epochs=50, plotting=True,print_period=None)\n",
    "    timer[i,1] = time.time()-start\n",
    "    \n",
    "    # here I am borrowing Keras' model to evaluate the loss function of weights from Hessian free\n",
    "    model.set_weights(pack_weights(ff))\n",
    "    \n",
    "     # evaluation metrics\n",
    "    pred = model.predict(X_test).flatten()\n",
    "    truth = ret_test.flatten()\n",
    "    scores[i,:,1] = [ f(pred,truth) for j,f in enumerate(eval_f) ]\n",
    "    \n",
    "\n",
    "\n",
    "# print 'keras training loss',hist.history['loss']\n",
    "# print 'valdidation loss',hist.history['val_loss']\n",
    "# print 'Hessian Free training loss',ff.optimizer.plots['training error (log)'] # it says log but it's not for MSE\n",
    "# print 'Hessian Free validation loss',ff.test_errs\n",
    "\n",
    "for jj in range(2):\n",
    "    print \n",
    "    exp = 'keras adagrad,hessian free'.split(',')[jj]\n",
    "    print('Evaluating ',exp)\n",
    "    print('running time per trial',timer[:,jj])\n",
    "    s = scores[:,:,jj]\n",
    "    print('prediction scores')\n",
    "    \n",
    "    mu = s.mean(axis=0)\n",
    "    sd = s.std(axis=0)\n",
    "\n",
    "    lower_bound = np.percentile(s, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(s, 97.5, axis=0)\n",
    "     \n",
    "    for i in range(s.shape[1]):\n",
    "        print(labels[i])\n",
    "        ##print('mean {} and std {}'.format(mu[i],std[i]))\n",
    "        print('2.5 and 97.5 percentile [{},{}]'.format(lower_bound[i],upper_bound[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ec9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
